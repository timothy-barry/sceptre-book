# A deeper dive into the Nextflow pipeline

In this chapter we discuss several technical aspects of the `sceptre` Nextflow pipeline. First, we review computing clusters and explain how information flows through the `sceptre` Nextflow pipeline on a cluster. Next, we describe the structure of a `sceptre` Nextflow submission script. Finally, we provide a few examples of common analysis workflows in the context of the Nextflow pipeline.

## Cluster architecture and Nextflow pipeline structure

A computing cluster is of a collection of computers, also known as *nodes*, that work in concert to carry out a large computation. @fig-cluster_structure illustrates the structure of a typical computing cluster. The cluster contains dozens or even hundreds of nodes, with each node harboring multiple *processors*. Processors are the core units that execute code. Furthermore, all nodes are linked to a common storage area where files are kept. For instance, specific data files like `gene.odm`, `grna.odm`, and `sceptre_object.rds` are stored in this shared space.

The process of performing a computation on a cluster involves several steps. Initially, each processor loads only the part of the data that it needs for its portion of the overall computation. Then, each processor processes this part of the data and saves the results back to the shared storage area. Once all processors complete their tasks, another processor retrieves these results from the shared space and merges them into a final result. Typically, the processors are spread across different nodes and thus do not directly interact with each other; rather, they coordinate through the shared storage space. This method of distributing tasks among independent processors and later combining their outputs is known as a *scatter-gather* operation. The `sceptre` Nextflow pipeline consists of multiple scatter-gather chained together.

```{r, out.width = "450px", fig.align="center", echo = FALSE}
#| label: fig-cluster_structure
#| fig-cap: "A model of the architecture of a standard computing cluster. There are several nodes, each of which contains multiple processors and is connected to the shared storage space. The data --- as stored within the `gene.odm`, `grna.odm`, and `sceptre_object.rds` files --- are contained in the shared storage space."
knitr::include_graphics("cluster_structure.png")
```

A graph illustrating the flow of information through the `sceptre` Nextflow pipeline is depicted in @fig-pipeline_dag. The steps of the pipeline as outlined in @fig-nextflow_pipeline_schematic (i.e., "set analysis parameters," "assign gRNAs," "run quality control," "run calibration check," "run power check," and "run discovery analysis") are all present in the graph. The computation is parallelized at four points: "assign gRNAs," "run calibration check," "run power check," and "run discovery analysis." The "assign gRNAs" step is parallelized according to the following scatter-gather strategy. First, the gRNAs are partitioned into *p* distinct "pods" (where the number of pods is selected by the user). For example, if the data contain 100 gRNAs, and if the user chooses to use *p* = 5 pods, then each pod is assigned 20 distinct gRNAs. Next, each pod is mapped to one of *p* processors, and each processor performs the gRNA-to-cell assignments for the gRNAs within the pod to which it has been assigned. Finally, the gRNA-to-cell assignments are combined across pods, yielding a single binary gRNA-to-cell assignment matrix.

```{r, out.width = "650px", fig.align="center", echo = FALSE}
#| label: fig-pipeline_dag
#| fig-cap: "A graph illustrating the flow of information through the `sceptre` Nextflow pipeline. The pipeline consists of several scatter-gather operations chained together, with parallelization at the gRNA assignment and association testing steps."
knitr::include_graphics("pipeline_dag.png")
```

The "run calibration check," "run power check," and "run discovery analysis" steps are parallelized using a similar strategy. First, the target-response pairs are partitioned into *r* distinct pods (where, again, *r* is selected by the user). Next, the pods are mapped to *r* processors, and each processor carries out the target-to-response association tests for the pairs in the pod to which it has been assigned. Finally, the results are combined across pods, and a multiplicity correction (e.g., the BH procedure) is applied to the entire set of p-values. In summary the gRNA-to-cell assignments are parallelized at the level of the gRNA, and the target-to-response association tests are parallelized at the level of the target-response pair. (If the "maximum" gRNA assignment strategy is used, then the gRNA-to-cell assignment step is not parallelized. Rather, the gRNA-to-cell assignments are carried out "behind the scenes" as part of the data import step.)

## Pipeline launch script

An example `sceptre` Nextflow pipeline launch script is as follows.

``` {.bash filename="launch_script.sh"}
#!/bin/bash
# Limit NF driver to 4 GB memory
export NXF_OPTS="-Xms500M -Xmx4G"

##########################
# REQUIRED INPUT ARGUMENTS
##########################
data_directory=$HOME"/sceptre_data/"
# sceptre object
sceptre_object_fp=$data_directory"sceptre_object.rds"
# response ODM
response_odm_fp=$data_directory"gene.odm"
# grna ODM
grna_odm_fp=$data_directory"grna.odm"

###################
# OUTPUT DIRECTORY:
##################
output_directory=$HOME"/sceptre_outputs"

#################
# Invoke pipeline
#################
nextflow run timothy-barry/sceptre-pipeline -r main \
 --sceptre_object_fp $sceptre_object_fp \
 --response_odm_fp $response_odm_fp \
 --grna_odm_fp $grna_odm_fp \
 --output_directory $output_directory \
 --grna_assignment_method mixture \
 --response_n_umis_range_lower 0.05 \
 --response_n_umis_range_uppper 0.95 \
 --grna_integration_strategy singleton \
 --pair_pod_size 1000 \
 --grna_pod_size 25 \
 --combine_assign_grnas_memory 8GB \
 --combine_assign_grnas_time 5m
```

We describe each component of this script line-by-line. First, we limit the Nextflow driver to four gigabytes of memory by including the line `export NXF_OPTS="-Xms500M -Xmx4G"`. (This line is optional but helps prevent the driver from exceeding its memory allocation.) Next, we set the variables `sceptre_object_fp`, `response_odm_fp`, and `grna_odm_fp` to the file paths of the `sceptre_object.rds` file, the `gene.odm` file, and the `grna.odm` file, respectively. We also set `output_directory` to the file path of the directory in which to write the results. These variables are defined in terms of `$HOME` as opposed to the shorthand `~`, as `$HOME` contains the absolute file path to the home directory, and Nextflow is more adept at handling absolute file paths than relative file paths.

Finally, we invoke the Nextflow pipeline via the command `nextflow run timothy-barry/sceptre-pipeline -r main`. (`timothy-barry/sceptre-pipeline` is the current location of the `sceptre` Nextflow pipeline on Github; we plan to move the pipeline into the [nf-core](https://nf-co.re/pipelines) repository in the future.) We specify arguments to the pipeline via the syntax `--argument value`, where `argument` is the name of an argument and `value` is the value to assign to the argument. In the above script we organize the arguments such that each argument is on its own line, but this is not necessary. The `sceptre` pipeline has four required arguments: `sceptre_object_fp`, `response_odm_fp`, `grna_odm_fp`, and `output_directory` (described above). The pipeline additionally supports a variety of optional arguments. About half of the optional arguments are "statistical" in that they govern how the statistical methods are to be deployed to analyze the data. For example, above, we indicate that the pipeline is to use the "mixture" method to assign gRNAs to cells via `--grna_assignment_method`. Additionally, we alter the default cellwise QC thresholds via `--response_n_umis_range_lower 0.05` and `--response_n_umis_range_uppper 0.95`. Finally, we indicate that the pipeline is to use the "singleton" gRNA integration strategy via `--grna_integration_strategy singleton`.

The other half of the optional arguments are "computational" in that they control how the computation is organized and executed. For example, we set the gRNA pod size to 25 and the target-response pair pod size to 1,000 via `–pair_pod_size 1000` and `–-grna_pod_size 25`, respectively. (The gRNA pod size and pair pod size correspond to *p* and *r* in @fig-pipeline_dag). Next, each process in the pipeline submits a time and memory allocation request to the scheduler. These resource requests are set to reasonable defaults, but we manually can override the defaults for a given process (or processes) on the command line. For example, we request eight gigabytes of memory and five minutes of wall time for the "Combine assign gRNAs" process via `–-combine_assign_grnas_memory 8GB` and `–-combine_assign_grnas_time 5m`, respectively.

::: callout-tip
All of the `sceptre` Nextflow pipeline arguments are enumerated and described in @sec-nf_pipeline_args of the addendum to Part II of this book. We recommend that users skim through this section and reference it as needed.
:::

## Example workflows

We describe a few common workflows within the context of the `sceptre` Nextflow pipeline.

### Tweaking the pipeline arguments on the basis of the intermediate results 

A common workflow is to advance through the pipeline one step at a time (@sec-incremental_pipeline), adjusting the input arguments as necessary based on the intermediate results. For example, suppose that we decide to implement an aggressive cellwise QC strategy, setting `--response_n_umis_range_lower` and `--response_n_umis_range_uppper` to 0.2 and 0.8, respectively, so as to clip the tails of the `response_n_umis` distribution at the 20th percentile and 80 percentile. We set `--pipeline_stop` to `run_qc` to stop the pipeline at the "Run QC" step.

```{bash,eval=FALSE}
#| code-fold: true
#| code-summary: "Show code"
#!/bin/bash
# Limit NF driver to 4 GB memory
export NXF_OPTS="-Xms500M -Xmx4G"

##########################
# REQUIRED INPUT ARGUMENTS
##########################
data_directory=$HOME"/sceptre_data/"
# sceptre object
sceptre_object_fp=$data_directory"sceptre_object.rds"
# response ODM
response_odm_fp=$data_directory"gene.odm"
# grna ODM
grna_odm_fp=$data_directory"grna.odm"

###################
# OUTPUT DIRECTORY:
##################
output_directory=$HOME"/sceptre_outputs"

#################
# Invoke pipeline
#################
nextflow run timothy-barry/sceptre-pipeline -r main -resume \
 --sceptre_object_fp $sceptre_object_fp \
 --response_odm_fp $response_odm_fp \
 --grna_odm_fp $grna_odm_fp \
 --output_directory $output_directory \
 --pair_pod_size 1000 \
 --grna_pod_size 25 \
 --response_n_umis_range_lower 0.2 \
 --response_n_umis_range_uppper 0.8 \
 --pipeline_stop run_qc
```



```{r, out.width = "600px", fig.align="center", echo = FALSE}
#| fig-cap: "Aggressive cellwise QC strategy."
knitr::include_graphics("plot_covariates_aggressive.png")
```


### Debugging the pipeline

The Nextflow pipeline may fail. This most commonly occurs when a process has exceeded its time and/or memory allocation. We provide an example of a pipeline failure and show how to resolve the failure. Consider the following submission script, in which we request only one MB of memory for the "Run association analysis" processes.

```{bash,eval=FALSE}
#| code-fold: true
#| code-summary: "Show code"
#!/bin/bash
# Limit NF driver to 4 GB memory
export NXF_OPTS="-Xms500M -Xmx4G"

##########################
# REQUIRED INPUT ARGUMENTS
##########################
data_directory=$HOME"/sceptre_data/"
# sceptre object
sceptre_object_fp=$data_directory"sceptre_object.rds"
# response ODM
response_odm_fp=$data_directory"gene.odm"
# grna ODM
grna_odm_fp=$data_directory"grna.odm"

###################
# OUTPUT DIRECTORY:
##################
output_directory=$HOME"/sceptre_outputs"

#################
# Invoke pipeline
#################
nextflow run timothy-barry/sceptre-pipeline -r main -resume \
 --sceptre_object_fp $sceptre_object_fp \
 --response_odm_fp $response_odm_fp \
 --grna_odm_fp $grna_odm_fp \
 --output_directory $output_directory \
 --pair_pod_size 1000 \
 --grna_pod_size 25 \
 --run_association_analysis_memory 1MB
```

We submit this script to the scheduler.

``` {.terminal filename="terminal, cluster"}
qsub launch.sh # or sbatch, etc
```

The job fails with the following log file.

```{r, out.width = "700px", fig.align="center", echo = FALSE}
#| fig-cap: "Log file for failed Nextflow pipeline."
knitr::include_graphics("nf_output_3.png")
```

We see that there was an error executing the process `run_analysis_subworkflow_calibration_check:run_association_analysis`, which is the calibration check process. Moreover, the process failed with exit code of 140, which, on an SGE cluster, indicates that the process exceeded its memory allocation. To resolve this issue, we increase the memory request of the "Run association analysis" processes to `4GB`, as follows.

```{bash,eval=FALSE}
#| code-fold: true
#| code-summary: "Show code"
#!/bin/bash
# Limit NF driver to 4 GB memory
export NXF_OPTS="-Xms500M -Xmx4G"

##########################
# REQUIRED INPUT ARGUMENTS
##########################
data_directory=$HOME"/sceptre_data/"
# sceptre object
sceptre_object_fp=$data_directory"sceptre_object.rds"
# response ODM
response_odm_fp=$data_directory"gene.odm"
# grna ODM
grna_odm_fp=$data_directory"grna.odm"

###################
# OUTPUT DIRECTORY:
##################
output_directory=$HOME"/sceptre_outputs"

#################
# Invoke pipeline
#################
nextflow run timothy-barry/sceptre-pipeline -r main -resume \
 --sceptre_object_fp $sceptre_object_fp \
 --response_odm_fp $response_odm_fp \
 --grna_odm_fp $grna_odm_fp \
 --output_directory $output_directory \
 --pair_pod_size 1000 \
 --grna_pod_size 25 \
 --run_association_analysis_memory 4GB
```

Now, the pipeline runs without issue. Determining the exact cause of a pipeline failure may take some sleuthing. In addition to examining the log file associated with the driver process, investigating the specific job that failed --- through, e.g., a call to `qacct` on SGE or `sacct` on SLURM --- can provide helpful information.

### Skipping the calibration check

One can skip the calibration check and proceed straight to the power check and discovery analysis by setting `--n_calibration_pairs` to `0`, as follows.

```{bash,eval=FALSE}
#| code-fold: true
#| code-summary: "Show code"
#!/bin/bash
# Limit NF driver to 4 GB memory
export NXF_OPTS="-Xms500M -Xmx4G"

##########################
# REQUIRED INPUT ARGUMENTS
##########################
data_directory=$HOME"/sceptre_data/"
# sceptre object
sceptre_object_fp=$data_directory"sceptre_object.rds"
# response ODM
response_odm_fp=$data_directory"gene.odm"
# grna ODM
grna_odm_fp=$data_directory"grna.odm"

###################
# OUTPUT DIRECTORY:
##################
output_directory=$HOME"/sceptre_outputs"

#################
# Invoke pipeline
#################
nextflow run timothy-barry/sceptre-pipeline -r main -resume \
 --sceptre_object_fp $sceptre_object_fp \
 --response_odm_fp $response_odm_fp \
 --grna_odm_fp $grna_odm_fp \
 --output_directory $output_directory \
 --pair_pod_size 1000 \
 --grna_pod_size 25 \
 --n_calibration_pairs 0
```
